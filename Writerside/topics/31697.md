# OpenAI forms new team to assess ‘catastrophic risks’ of AI
**Emma Roth**

**2023-10-26 19:52**

**https://www.theverge.com/2023/10/26/23933783/openai-preparedness-team-catastrophic-risks-ai**

OpenAI’s new preparedness team will address the potential dangers associated with AI, including nuclear threats.
----------------------------------------------------------------------------------------------------------------

![](https://cdn.vox-cdn.com/thumbor/QCdO_oKAdBIZCodEUT2WCBFgQSo=/0x0:2040x1360/1200x628/filters:focal(1020x680:1021x681)/cdn.vox-cdn.com/uploads/chorus_asset/file/24390406/STK149_AI_03.jpg)

OpenAI is forming a new team to mitigate the “catastrophic risks” associated with AI. In [an update on Thursday](https://openai.com/blog/frontier-risk-and-preparedness), OpenAI says the preparedness team will “track, evaluate, forecast, and protect” against potentially major issues caused by AI, including nuclear threats.

The team will also work to mitigate “chemical, biological, and radiological threats,” as well as “autonomous replication,” or the act of an AI replicating itself. Some other risks that the preparedness team will address include AI’s ability to trick humans, as well as cybersecurity threats.

“We believe that frontier AI models, which will exceed the capabilities currently present in the most advanced existing models, have the potential to benefit all of humanity,” OpenAI writes in the update. “But they also pose increasingly severe risks.”

Aleksander Madry, [who is currently on leave](https://madry.mit.edu/) from his role as the director of MIT’s Center for Deployable Machine Learning, will lead the preparedness team. OpenAI notes that the preparedness team will also develop and maintain a “risk-informed development policy,” which will outline what the company is doing to evaluate and monitor AI models.

OpenAI CEO Sam Altman has warned of the potential for catastrophic events caused by AI before. In May, [Altman and other prominent AI researchers](https://www.theverge.com/2023/5/30/23742005/ai-risk-warning-22-word-statement-google-deepmind-openai) issued a 22-word statement that “mitigating the risk of extinction from AI should be a global priority.” During an interview in London, [Altman also suggested that governments](https://www.theverge.com/2023/5/24/23735982/sam-altman-openai-superintelligent-benefits-talk-london-ucl-protests) should treat AI “as seriously” as nuclear weapons.

Comments